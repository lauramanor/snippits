{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_environment_parser.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lauramanor/snippits/blob/master/simple_environment_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqIsSQieqUf4",
        "colab_type": "text"
      },
      "source": [
        "# This is a simple parser for finding minimal pairs and environments\n",
        "\n",
        "You will need to upload your file to your google drive. \n",
        "\n",
        "When prompted, click on the link to get authentication to allow Google to access your Drive. You should see a screen with “Google Cloud SDK wants to access your Google Account” at the top. After you allow permission, copy the given verification code and paste it in the box in Colab.\n",
        "\n",
        "Once you have completed verification, go to the CSV file in Google Drive,right-click on it and select “Get shareable link”. The link will be copied into your clipboard. Paste this link into a string variable in Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlD73p3weNML",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import pandas as pd\n",
        "from pandas import ExcelWriter\n",
        "from pandas import ExcelFile\n",
        "from collections import defaultdict\n",
        "\n",
        "import difflib\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg7Ps43TvbeH",
        "colab_type": "text"
      },
      "source": [
        "## Enter applicable information necessary here:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfD1f4bAj6zH",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "\n",
        "\n",
        "#@markdown  Enter Sharable Link for your xcls here: \n",
        "link = \"https://drive.google.com/open?id=1X-owhxYxF6AhQra8IRAtaqNTVN5u2Lav\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown  Enter the name of your file here, it should include any extensions\n",
        "filename = 'ANG-201907-Elicitation Master.csv' #@param {type:\"string\"}\n",
        "#@markdown  Enter the sheetname here (for xlsx)\n",
        "sheetname = '' #@param {type:\"string\"}\n",
        "#@markdown  Enter the row that contains the headding info\n",
        "heading_row = 7 #@param {type:\"integer\"}\n",
        "#@markdown  Enter the data column. MUST BE EXACT!!!  (currently supoports only one column)\n",
        "data_column = \"Angaite 9/2019\" #@param {type:\"string\"}\n",
        "#@markdown  Enter the id columns(s). MUST BE EXACT!!!  (sperate columns with tabs)\n",
        "header_columns = \"Date\\tSession\\tItem#\" #@param {type:\"string\"}\n",
        "\n",
        "header_columns = header_columns.split(\"\\t\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EipWGXdErDlF",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1Mylvy4Yyp0ZFM8vr_SI2H2hlc5dN3IgU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb7Mgjmzlkvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "cellView": "both",
        "outputId": "1996d128-f168-4e9a-b057-a62d4631f24b"
      },
      "source": [
        "#@title Upload File\n",
        "\n",
        "if \"id\" in link:\n",
        "  fluff, id = link.split('id=')\n",
        "elif \"/d/\" in link:\n",
        "  fluff, id = link.split(\"/d/\")\n",
        "else:\n",
        "  print(\"Link not supported\")\n",
        "  raise SystemExit\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile(filename)  \n",
        "if filename.endswith(\".xlsx\"):\n",
        "  df = pd.read_excel(filename, sheet_name=sheetname, header=heading_row-1)\n",
        "elif filename.endswith(\".csv\"):\n",
        "    df = pd.read_csv(filename, header=heading_row-1 )\n",
        "\n",
        "print(header_columns)\n",
        "g = [df[x].fillna(\"0\").astype(int).astype(str) for x in header_columns]\n",
        "# print(g)\n",
        "uid = pd.concat(g,axis=1 ).apply(lambda x: '_'.join(x) if not x.all() == '' else x.sum(),axis=1)\n",
        "df[\"uid\"] = uid"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Date', 'Session', 'Item#']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xWWWcWCvugb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Check to make sure this looks like the data you want to parse:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGWsoQRCvyp7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "cellView": "both",
        "outputId": "9265c438-a51c-4e8d-c2f1-56463f9490f9"
      },
      "source": [
        "df[[data_column, \"uid\"]].head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Angaite 9/2019</th>\n",
              "      <th>uid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(kilkaske?) nepkeˈsek</td>\n",
              "      <td>20190716_1_23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(Lea) aklooˈma</td>\n",
              "      <td>20190708_1_80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aaˈLa</td>\n",
              "      <td>20190717_2_39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aaˈLa aLwasekˈsek</td>\n",
              "      <td>20190710_2_63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aaˈLa aLwasekˈsek nentoˈma</td>\n",
              "      <td>20190723_1_43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>aaLajkoˈ'ok</td>\n",
              "      <td>20190729_0_12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>aaˈmaj (aˈmaj?)</td>\n",
              "      <td>20190708_2_34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>aaˈnik</td>\n",
              "      <td>20190720_2_42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>aaˈwa (awa?)</td>\n",
              "      <td>20190723_1_26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ahanˈkok</td>\n",
              "      <td>20190723_1_54</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Angaite 9/2019            uid\n",
              "0       (kilkaske?) nepkeˈsek  20190716_1_23\n",
              "1              (Lea) aklooˈma  20190708_1_80\n",
              "2                       aaˈLa  20190717_2_39\n",
              "3           aaˈLa aLwasekˈsek  20190710_2_63\n",
              "4  aaˈLa aLwasekˈsek nentoˈma  20190723_1_43\n",
              "5                 aaLajkoˈ'ok  20190729_0_12\n",
              "6             aaˈmaj (aˈmaj?)  20190708_2_34\n",
              "7                      aaˈnik  20190720_2_42\n",
              "8                aaˈwa (awa?)  20190723_1_26\n",
              "9                    ahanˈkok  20190723_1_54"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MlCH7-Kz_Bw",
        "colab_type": "text"
      },
      "source": [
        "## Are there any multi-glyph phones we should know about?\n",
        "\n",
        "for example, if you want sh in ship to count as one, note that below. Each groupone should be separated with a space\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbcBCN0q0Qdy",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title multi-glyph\n",
        "\n",
        "multi_glyphs = \"\" #@param {type:\"string\"}\n",
        "\n",
        "multi_glyphs = multi_glyphs.split(\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKDA4yW71Zxq",
        "colab_type": "text"
      },
      "source": [
        "## Basica Parse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG2DGrhzBQSI",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title create special dictionary\n",
        "\n",
        "\n",
        "class ThingDict(object):\n",
        "  \"\"\"\n",
        "    blueprint doing stuff with these words\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, idx_adjust, ):\n",
        "    self.keys = []\n",
        "    self.data_idxs = []\n",
        "    self.idx_adjust = idx_adjust + 1\n",
        "    self.uids = []\n",
        "\n",
        "  def add(self, key, new_value, uid):\n",
        "    adjusted_value = new_value + self.idx_adjust\n",
        "    if key in self.keys:\n",
        "      idx = self.keys.index(key)\n",
        "      self.data_idxs[idx].append(adjusted_value)\n",
        "      self.uids[idx].append(uid)\n",
        "\n",
        "    else:\n",
        "      self.keys.append(key)\n",
        "      self.data_idxs.append([adjusted_value])\n",
        "      self.uids.append([uid])\n",
        "\n",
        "\n",
        "\n",
        "  def keys(self):\n",
        "    return self.keys\n",
        "\n",
        "  def get_idxs(self, key):\n",
        "    idx = self.keys.index(key)\n",
        "    return self.data_idxs[idx]\n",
        "\n",
        "  def get_uids(self, key):\n",
        "    idx = self.keys.index(key)\n",
        "    return self.uids[idx]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiS7op851Z_W",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title do a simple parse for word boundaries, add words to dict, and remove anything in parens  \n",
        "\n",
        "def split_word_boundary(data_str):\n",
        "  data_str = str(data_str)\n",
        "  wrds = data_str.split(\" \")\n",
        "  to_remove = []\n",
        "  for wrd in wrds:\n",
        "    if \"(\" in wrd:\n",
        "        to_remove.append(wrd)\n",
        "    elif \")\" in wrd:\n",
        "        to_remove.append(wrd)\n",
        "  for rm in to_remove:\n",
        "    wrds.remove(rm)\n",
        "  return wrds\n",
        "\n",
        "df[\"split\"] = df[data_column].apply(split_word_boundary)\n",
        "\n",
        "\n",
        "words_dict = ThingDict(heading_row)\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    # print(idx, row)\n",
        "    for r in row[\"split\"]:\n",
        "        # print(r, idx)\n",
        "        words_dict.add(r, idx, row[\"uid\"])\n",
        "        # print(words_dict.get_uids(r))\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7MicV4SzU1_",
        "colab_type": "text"
      },
      "source": [
        "## Get close matches\n",
        "\n",
        "Similarity score refers to how similar the words must be (0,1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP4a72oyCDrB",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title close matches\n",
        "\n",
        "import pprint\n",
        "import json\n",
        "import csv\n",
        "import io\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "def pretty_print_matches(matches):\n",
        "    for key in matches:\n",
        "        print(f\"{key}: {matches[key]['idxs']}\")\n",
        "        to_sort = matches[key]\n",
        "        for match in sorted(to_sort, key=lambda k: len(to_sort[k]), reverse=True):\n",
        "            if match != \"idxs\":\n",
        "                print(f\"\\t{match}: {matches[key][match]}\")\n",
        "\n",
        "def save_as_csv(matches):\n",
        "    output = io.StringIO()\n",
        "\n",
        "    writer = csv.writer(output, delimiter=\",\", quotechar='\"', )\n",
        "    writer.writerow([\"word1\", \"indexes1\", \"word2\", \"indexes2\"])\n",
        "    for key in matches:\n",
        "        for match in matches[key]:\n",
        "            if match != \"idxs\":\n",
        "                writer.writerow([key, matches[key]['idxs'], match, matches[key][match]])\n",
        "\n",
        "    print(f\"\\nCOPY AFTER THIS: \\n{output.getvalue()}\\n\")\n",
        "\n",
        "def close_matches(thing_dict, one_word=None, print_me=True, print_csv=False, similarity_score=.8):\n",
        "    all_words = thing_dict.keys\n",
        "    to_return = dict()\n",
        "    if one_word:\n",
        "        test_words = [one_word]\n",
        "    else:\n",
        "        test_words = all_words\n",
        "\n",
        "    for test in test_words:\n",
        "        if len(test) > 0:\n",
        "            matches = difflib.get_close_matches(test, all_words, n=10, cutoff=similarity_score)\n",
        "            match_dict = dict()\n",
        "            for match in matches:\n",
        "                if match != test:\n",
        "                    match_dict[match] = thing_dict.get_uids(match)\n",
        "            match_dict[\"idxs\"] = thing_dict.get_uids(test)\n",
        "            to_return[test] = match_dict\n",
        "\n",
        "    if print_me:\n",
        "        pretty_print_matches(to_return)\n",
        "\n",
        "    if print_csv:\n",
        "        save_as_csv(to_return)\n",
        "\n",
        "    return to_return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtgwDUPyYf5A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b822aeab-ee70-4674-9c5f-ba54af97b667"
      },
      "source": [
        "one_close_match = close_matches(words_dict, one_word=\"taaˈta\", print_csv=True)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "taaˈta: ['20190710_2_17']\n",
            "\ttaaˈLa: ['20190720_1_1', '20190720_1_2', '20190708_1_1']\n",
            "\tnaaˈta: ['20170722_1_11', '20190715_1_20', '20190716_1_14']\n",
            "\tjatanaˈta: ['20190715_2_43', '20190715_2_44', '20190715_2_45']\n",
            "\ttaaˈma: ['20190722_2_70', '20190710_2_54']\n",
            "\tjetanaˈta: ['20190722_1_12']\n",
            "\n",
            "COPY AFTER THIS: \n",
            "word1,indexes1,word2,indexes2\r\n",
            "taaˈta,['20190710_2_17'],taaˈma,\"['20190722_2_70', '20190710_2_54']\"\r\n",
            "taaˈta,['20190710_2_17'],taaˈLa,\"['20190720_1_1', '20190720_1_2', '20190708_1_1']\"\r\n",
            "taaˈta,['20190710_2_17'],naaˈta,\"['20170722_1_11', '20190715_1_20', '20190716_1_14']\"\r\n",
            "taaˈta,['20190710_2_17'],jetanaˈta,['20190722_1_12']\r\n",
            "taaˈta,['20190710_2_17'],jatanaˈta,\"['20190715_2_43', '20190715_2_44', '20190715_2_45']\"\r\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4d3ebF5i7hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "all_close_matches = close_matches(words_dict, print_me=False, print_csv=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxd6vJwvw9KZ",
        "colab_type": "text"
      },
      "source": [
        "## Now, let's find some environments\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BHLgHjgaM5P8",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title some stuff\n",
        "\n",
        "\n",
        "\n",
        "def pretty_print_envs(envs, max_examples=100):\n",
        "    for pattern in envs:\n",
        "        for thing in envs[pattern]:\n",
        "            print(f\"{thing.upper()} environments for {pattern} (envs={len(envs[pattern][thing])})\")\n",
        "            to_sort = envs[pattern][thing]\n",
        "            for env in sorted(to_sort, key=lambda k: len(to_sort[k]), reverse=True):\n",
        "                print(f\"\\t {env} (examples={len(envs[pattern][thing][env])})\")\n",
        "                examples = envs[pattern][thing][env]\n",
        "                examples.sort(key=lambda t: len(t[1]), reverse=True)\n",
        "                for example in examples[:max_examples]:\n",
        "                    print(f\"\\t\\t{example}\")\n",
        "\n",
        "def prep(ngram, pattern):\n",
        "    return ngram.replace(pattern, \"_\")\n",
        "\n",
        "def find_environments(words_dict,patterns,left=True, right=True, n=2,print_me=False):\n",
        "\n",
        "    patterns_dict = dict((p, {}) for p in patterns)\n",
        "\n",
        "    for pattern in patterns:\n",
        "        starts = defaultdict(list)\n",
        "        ends = defaultdict(list)\n",
        "        lefts = defaultdict(list)\n",
        "        rights = defaultdict(list)\n",
        "        alls = defaultdict(list)\n",
        "        for word in words_dict.keys:\n",
        "\n",
        "            if pattern in word:\n",
        "                if len(pattern) + n < len(word):\n",
        "                    ngrams = [\"\".join(ngram) for ngram in zip(*[word[i:] for i in range(len(pattern)+n)])]\n",
        "                else: \n",
        "                    ngrams = [word]\n",
        "\n",
        "                if word.startswith(pattern):\n",
        "                    starts[prep(ngrams[0], pattern)].append((word, words_dict.get_uids(word)))\n",
        "\n",
        "                if word.endswith(pattern):\n",
        "                    ends[prep(ngrams[-1], pattern)].append((word, words_dict.get_uids(word)))\n",
        "\n",
        "                for ngram in ngrams:\n",
        "                    if pattern in ngram:\n",
        "                        alls[prep(ngram, pattern)].append((word, words_dict.get_uids(word)))\n",
        "                        if ngram.endswith(pattern):\n",
        "                            lefts[prep(ngram, pattern)].append((word, words_dict.get_uids(word)))\n",
        "                        if ngram.startswith(pattern):\n",
        "                            rights[prep(ngram, pattern)].append((word, words_dict.get_uids(word)))\n",
        "\n",
        "\n",
        "        patterns_dict[pattern][\"all\"] = dict(alls)\n",
        "        patterns_dict[pattern][\"start\"] = dict(starts)\n",
        "        patterns_dict[pattern][\"end\"] = dict(ends)\n",
        "        patterns_dict[pattern][\"right\"] = dict(rights)\n",
        "        patterns_dict[pattern][\"left\"] = dict(lefts)\n",
        "        \n",
        "\n",
        "    if print_me:\n",
        "        pretty_print_envs(patterns, max_examples=4)\n",
        "\n",
        "    return(patterns_dict)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsQ_dQsdL7mP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "8d2da67a-f2b5-4b0f-eb53-0147f7767b19"
      },
      "source": [
        "\n",
        "envs = find_environments(words_dict, patterns=[\"nepk\"], print_me=False )\n",
        "pretty_print_envs(envs)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ALL environments for nepk (envs=2)\n",
            "\t _eˈ (examples=1)\n",
            "\t\t('nepkeˈsek', ['20190716_1_23', '20190716_1_27', '20190716_1_57', '20190716_1_58', '20190708_3_45', '20190722_2_35', '20190722_2_37', '20190708_3_44', '20190716_1_30'])\n",
            "\t _ee (examples=1)\n",
            "\t\t('nepkeeˈsek', ['20190719_1_35', '20190708_3_43', '20190708_3_43'])\n",
            "START environments for nepk (envs=2)\n",
            "\t _eˈ (examples=1)\n",
            "\t\t('nepkeˈsek', ['20190716_1_23', '20190716_1_27', '20190716_1_57', '20190716_1_58', '20190708_3_45', '20190722_2_35', '20190722_2_37', '20190708_3_44', '20190716_1_30'])\n",
            "\t _ee (examples=1)\n",
            "\t\t('nepkeeˈsek', ['20190719_1_35', '20190708_3_43', '20190708_3_43'])\n",
            "END environments for nepk (envs=0)\n",
            "RIGHT environments for nepk (envs=2)\n",
            "\t _eˈ (examples=1)\n",
            "\t\t('nepkeˈsek', ['20190716_1_23', '20190716_1_27', '20190716_1_57', '20190716_1_58', '20190708_3_45', '20190722_2_35', '20190722_2_37', '20190708_3_44', '20190716_1_30'])\n",
            "\t _ee (examples=1)\n",
            "\t\t('nepkeeˈsek', ['20190719_1_35', '20190708_3_43', '20190708_3_43'])\n",
            "LEFT environments for nepk (envs=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDj_PgrvNHTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}